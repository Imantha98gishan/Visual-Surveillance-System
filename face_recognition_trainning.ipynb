{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import functools\n",
    "import numpy as np\n",
    "import random as rndm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import h5py\n",
    "import sys\n",
    "import threading\n",
    "import glob\n",
    "from IPython import display as ipythondisplay\n",
    "from string import Formatter\n",
    "import time\n",
    "from datetime import date\n",
    "from mtcnn import MTCNN\n",
    "import socket\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_images():  \n",
    "  model = tf.keras.models.load_model('face.h5') \n",
    "\n",
    "  # Create a test data generator\n",
    "  test_datagen = ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "  # Path to the test directory\n",
    "  test_dir = 'C:/Users/Gishan/Documents/HS/faces' \n",
    "\n",
    "  # Image dimensions\n",
    "  img_height, img_width = 150, 150 \n",
    "  batch_size = 32\n",
    "\n",
    "# Create test data generator\n",
    "  test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary', \n",
    "    shuffle=False \n",
    "  )\n",
    "\n",
    "# Get predictions\n",
    "  predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert probabilities to class labels (0 or 1)\n",
    "  predicted_classes = (predictions > 0.5).astype(int) \n",
    "\n",
    "# Get true class labels\n",
    "  true_labels = test_generator.classes\n",
    "\n",
    "# Evaluate model performance\n",
    "  accuracy = accuracy_score(true_labels, predicted_classes)\n",
    "  conf_matrix = confusion_matrix(true_labels, predicted_classes)\n",
    "  classification_rep = classification_report(true_labels, predicted_classes)\n",
    "\n",
    "# Print results \n",
    "  print(f\"Test Accuracy: {accuracy:.2f}\") \n",
    "  print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "  print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "  delete_files_in_directory('C:/Users/Gishan/Documents/HS/faces/user_1')\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files_in_directory(directory_path):\n",
    "    if not os.path.exists(directory_path): # Check if the directory exists\n",
    "        raise FileNotFoundError(f\"Directory '{directory_path}' not found.\") #Error if not\n",
    "\n",
    "    for filename in os.listdir(directory_path): # Loop through all files in the directory\n",
    "        file_path = os.path.join(directory_path, filename) # Get the full path of the file\n",
    "        os.remove(file_path) #remove the file\n",
    "\n",
    "def mse(imageA, imageB): # Function to calculate the mean squared error between two images\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2) \n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESP_IP = '192.168.8.188'  #IP of the ESP32 board\n",
    "ESP_PORT = 80 #port of the ESP board\n",
    "\n",
    "def send_data_to_esp(data):\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: \n",
    "      s.connect((ESP_IP, ESP_PORT)) #Connect to the ESP32 board\n",
    "      s.sendall(data.encode()) #Send data to the ESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userinput():\n",
    "    x=input('Enter any key to stop \\n') #user input\n",
    "    return x\n",
    "\n",
    "thread = threading.Thread(target=userinput) #Start thread to parallely to controll the process\n",
    "\n",
    "thread.start()\n",
    "\n",
    "cap = cv2.VideoCapture('http://192.168.8.168:81/stream') #capturing the stream\n",
    "\n",
    "detec = MTCNN()\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "current=date.today() #Grab the date\n",
    "\n",
    "data_directory = 'C:/Users/Gishan/Documents/HS/' #Directory to save the data\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  #Video record format\n",
    "print('recording..')\n",
    "\n",
    "n=0\n",
    "file_name = f'{current} {n}.mp4' \n",
    "file_path = os.path.join(data_directory, file_name)\n",
    "\n",
    "while os.path.exists(file_path)==True:\n",
    "    n+=1\n",
    "    file_name = f'{current} {n}.mp4' \n",
    "    file_path = os.path.join(data_directory, file_name)\n",
    "\n",
    "ind=0\n",
    "filename = f'img{ind}.jpg' \n",
    "data_directory = './frames'\n",
    "file_path = os.path.join(data_directory, filename)\n",
    "\n",
    "while os.path.exists(file_path)==True:\n",
    "    ind+=1\n",
    "    filename = f'img{ind}.jpg' \n",
    "    file_path = os.path.join(data_directory, filename)\n",
    "\n",
    "ind-=1\n",
    "\n",
    "out = cv2.VideoWriter(file_name, fourcc, 10.0, (640, 480)) #Video writer to save the video\n",
    "data = '1'\n",
    "\n",
    "num=0\n",
    "while thread.is_alive(): #if thread is alive \n",
    "    ret, frame = cap.read()\n",
    "    i=1\n",
    "\n",
    "    while not ret: #if there is no frame return\n",
    "        ret, frame = cap.read()\n",
    "        if i==1:\n",
    "            print('wait')\n",
    "            i+=1\n",
    "    \n",
    "    out.write(frame) #Write the frame to the video\n",
    "\n",
    "    if current != date.today(): #current date is different from the initial date\n",
    "        current = date.today() \n",
    "        out = cv2.VideoWriter(file_name, fourcc, 10.0, (640, 480)) #reintialize the details of the recording\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "    \n",
    "    faces = detec.detect_faces(frame) #detect faces in the frame\n",
    "\n",
    "    for face in faces:\n",
    "        x, y, w, h=face['box'] #get the position of the face\n",
    "        face_roi = frame[y:y+h, x:x+w] #draw the square\n",
    "        cv2.imwrite(f'C:/Users/Gishan/Documents/HS/faces/user_1/img{num}.jpg',face_roi) #store the face in local storage\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2) \n",
    "    \n",
    "    cv2.imshow('Camera', frame)    \n",
    "\n",
    "    num+=1    \n",
    "    if num==10: #count the frame till 10\n",
    "        num=0 #Reset\n",
    "        data = '1' #Reset the data\n",
    "        if len(os.listdir('./faces/user_1'))>0: #if there are faces in the local storage\n",
    "            accuracy=test_images()  #use the CNN model to test the faces\n",
    "            print(f'accuracy: {accuracy}')\n",
    "            if accuracy>=0.75: #if the accuracy is greater than 75%\n",
    "                data = '0' \n",
    "            else:\n",
    "                cv2.imwrite(f'C:/Users/Gishan/Documents/HS/frames/img{ind}.jpg',frame) #record the face in the local storage\n",
    "        send_data_to_esp(data) #send data to the ESP32\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset ESP\n",
    "send_data_to_esp('1')\n",
    "delete_files_in_directory('C:/Users/Gishan/Documents/HS/faces/user_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#misc code used to create validate directory\n",
    "detec = MTCNN()\n",
    "for i in range(100000, 100300):\n",
    "    prev=cv2.imread(f'./dataset/img_align_celeba/img_align_celeba/{i}.jpg')\n",
    "    faces = detec.detect_faces(prev)\n",
    "    \n",
    "    \n",
    "    for face in faces:\n",
    "        x, y, w, h=face['box']\n",
    "        face_roi = prev[y:y+h, x:x+w] \n",
    "        cv2.imwrite(f'./faces/intruder/{i}.jpg', face_roi)\n",
    "        \n",
    "\n",
    "cv2.waitKey(0)    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequential model to learn face features\n",
    "n_filters=16\n",
    "def learn_face(n_outputs=1):\n",
    "    #2D Layer which make 2D convolution\n",
    "    Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu') \n",
    "    #batch normalization rescalling\n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "    #Flatten the layer multi-dimension to single\n",
    "    Flatten = tf.keras.layers.Flatten\n",
    "    #dense layer fully connected layer\n",
    "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        Conv2D(filters=1*n_filters, kernel_size=5,  strides=2),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=2*n_filters, kernel_size=5,  strides=2),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=4*n_filters, kernel_size=5,  strides=2),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=8*n_filters, kernel_size=5,  strides=2),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=16*n_filters, kernel_size=5,  strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Flatten(),\n",
    "\n",
    "        Dense(512),\n",
    "        Dense(n_outputs, activation=None),\n",
    "    ])\n",
    "    return model\n",
    "face = learn_face()\n",
    "\n",
    "# Compile the model\n",
    "face.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect=MTCNN()\n",
    "#enroll new faces\n",
    "def enroll_faces():\n",
    "    n=1\n",
    "    x=True\n",
    "    cap = cv2.VideoCapture('http://192.168.8.168:81/stream') # capture video from the camera\n",
    "    precentage=0\n",
    "    while x:\n",
    "        ret, frame = cap.read()\n",
    "        faces = detect.detect_faces(frame)\n",
    "\n",
    "        for face in faces:\n",
    "            x, y, w, h=face['box']\n",
    "            face_roi = frame[y:y+h, x:x+w] \n",
    "            cv2.imwrite(f'./faces/user_1/img{precentage}.jpg', face_roi)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            precentage+=1\n",
    "        \n",
    "        cv2.imshow('frame', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        if precentage<=100:\n",
    "            print('=', end='')\n",
    "        else:\n",
    "            break\n",
    "              \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    img_height, img_width = 150, 150 \n",
    "    batch_size = 10\n",
    "\n",
    "    train_dir = './faces' \n",
    "    val_dir = './nonuser' \n",
    "\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load and preprocess images from directories\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "\n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "    history = face.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // batch_size,\n",
    "        epochs=10, \n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // batch_size\n",
    "    )\n",
    "\n",
    "# Evaluate the model\n",
    "    test_loss, test_acc = face.evaluate(validation_generator)\n",
    "    print('Test accuracy:', test_acc)\n",
    "\n",
    "    # Save the model\n",
    "    face.save('face.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('face.h5') #delete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a convolutional layer with glorot_uniform kernel\n",
    "conv_layer = tf.keras.layers.Conv2D(\n",
    "    filters=1,  # Single filter\n",
    "    kernel_size=(3, 3),  # 3x3 kernel\n",
    "    strides=(1, 1),  # Stride of 1\n",
    "    padding='same',  # Preserve spatial dimensions\n",
    "    kernel_initializer='glorot_uniform',  # Use glorot_uniform initializer\n",
    "    use_bias=False  # Disable bias for simplicity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic grayscale image (e.g., a checkerboard pattern)\n",
    "image = cv2.imread(f'C:/Users/Gishan/Documents/HS/frames/out2.jpg')\n",
    "\n",
    "# Display the original image\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(\"Original Image\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "image=image.astype(np.float32)\n",
    "# Build the layer by specifying the input shape\n",
    "input_image = tf.expand_dims(image,axis=0)# Add batch and channel dimensions\n",
    "conv_layer.build(input_image.shape)\n",
    "\n",
    "# Apply the convolutional layer to the image\n",
    "output_image = conv_layer(input_image)\n",
    "#output_image = conv_layer(output_image)\n",
    "#output_image = conv_layer(output_image)\n",
    "\n",
    "# Remove batch and channel dimensions for visualization\n",
    "output_image = tf.squeeze(output_image).numpy()\n",
    "cv2.imwrite(f'C:/Users/Gishan/Documents/HS/frames/out3.jpg',output_image)\n",
    "\n",
    "# Display the output image\n",
    "plt.imshow(output_image, cmap='gray')\n",
    "plt.title(\"Output Image (After Applying glorot_uniform Kernel)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Get the kernel weights\n",
    "kernel_weights = conv_layer.weights[0].numpy()\n",
    "\n",
    "# Visualize the kernel\n",
    "plt.imshow(output_image, cmap='gray')  # Visualize the first (and only) filter\n",
    "plt.title(\"glorot_uniform Kernel (3x3)\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
